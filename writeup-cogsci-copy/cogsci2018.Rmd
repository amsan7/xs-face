---
title: "Postural changes mediate children's visual access to social information"
bibliography: xsface.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Alessandro Sanchez} \\ \texttt{author1@university.edu} \\ Department of Psychology \\ Stanford University
    \And {\large \bf Bria Long} \\ \texttt{bria@stanford.edu} \\ Department of Psychology \\ Stanford University
        \And {\large \bf Ally Kraus} \\ \texttt{bria@stanford.edu} \\ Department of Psychology \\ Stanford University
        \And {\large \bf Michael C. Frank} \\ \texttt{mcfrank@stanford.edu} \\ Department of Psychology \\ Stanford University}

abstract: 
  "The ability to process social cues--including eye gaze--is a critical component of children's early language and cognitive development. However, as children reach their first birthday, they begin to locomote themselves, walking and exploring their visual environment in an entirely new way. How do these postural and locomotive changes affect children's access to the social information relevant for word-learning? Here, explore this question by using head-mounted cameras to record infants' (8-16 months of age) egocentric visual perspective and then using modern computer-vision algorithms to detect the proportion of faces and hands in infants' environments.  We find that infant's postural changes largely mediate infants' acess to both faces and hands, suggesting that these postural and locomotive developments facilitate infants' emerging linguistic and social capacities. Broadly, we suggest that the combined use of head-mounted cameras and the application of novel deep learning algorithms is a promising avenue for understanding the statistics of infants' visual and linguistic experience. "

keywords:
    "social cognition; face-perception; infancy; locomotion; head-cameras; deep learning"
    
output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(knitr)
knitr::opts_chunk$set(fig.width=6, fig.height=5, fig.crop = FALSE, 
                      fig.path='figs/', echo=FALSE, warning = FALSE, 
                      cache=TRUE, message = FALSE, sanitize = TRUE)
library(readr)
library(dplyr)
library(tidyr)
library(xtable)
library(lubridate)
library(langcog)
library(ggplot2)
library(lme4)
library(gridExtra)
```
# Introduction
Children are remarkably skilled language learners, connecting arbitrary labels (“cup”) with specific visual concepts at a rapid pace through the first two years of life. However, children do not learn words in a vacuum but in a rich social environment, where social cues provided by speakers (e.g., eye-gaze) provide strong scaffolding for this learning process. Indeed, children’s ability to effectively process these social cues may be a key factor in their early language development. For example,in a longitudinal study, children’s level of joint engagement with their mother was found to predict both their receptive and productive vocabularies (@carpenter1998).  More recently, 10 month-olds who follow an adult’s gaze in an experimental context have larger vocabularies at 18 months (@brooks2005) and throughout the second year of life (@brooks2008). 

However, as children are learning their first words, their view of the world is also changing radically [@adolph2007]. Infant’s motor abilities improve dramatically near the end of the first year of life,  allowing them to locomote independently and to determine what they see. These motor changes have drastic consequences for what children see: crawling and walking infants have radically different views of the world. During spontaneous play in a laboratory playroom, toddlers are more likely to look at the floor while crawling than while walking [@franchak2011]; walking vs. crawling infants tend to have full visual access to their environment and the people in it [@kretch2014].

One possibility is that these motor improvements have strong developmental cascades, impacting children’s emerging social, cognitive, and linguistic abilities (Iverson, 2010). Indeed, these postural changes also impact how children interact with their mothers;  walking infants make different kinds of object-related bids for attention from their mothers than crawling infants, and tend to hear more action directed statements (e.g., “open it”) [@karasik2014].  More directly, in an observational study, @walle2014 found that children who were able to walk had both higher receptive and productive vocabularies.  On their account, children’s ability to stand and independently locomote may fundamentally change their ability to access the social information (e.g., faces, gaze) relative to children who are still crawling and sitting. In other words, the ability to access more detailed social information may allow infants to learn words quicker and more efficiently, facilitating language growth.

Recent work has begun to use egocentric, head-mounted cameras to document the visual experiences of infants and children — which even for walking children are radically different than the adult perspective (and not easily predicted by our own adult intuitions; @franchak2011, @yoshida2008; @clerkin2017). Children’s views tend to be restricted and to be dominated by objects and hands much more than that of adults [@yoshida2008], and both computational and empirical work suggest that this restricted viewpoint may be more effective for learning what objects and their labels than the comparable adult perspective [@yurovsky2012, @bambach2017]. Further, recent work also suggests dramatic changes in the child’s perspective over the first two years of life, as views transition from primarily containing close up view of faces to capturing views of hands paired with the objects they are acting on [@fausey2016].

Here, we directly ask whether the postural changes that infants’ experience as they reach their first birthday change the availability of the social information relevant for word learning. To do so, we recorded the visual experience of a group of infants and children using head-mounted cameras during a brief laboratory free-play session; children’s posture and their orientation to their caregiver were also recorded from a third-person perspective and hand-annotated. We capitalize on recent improvements in computer vision algorithms (@zhang2016, @cao2017realtime) that allow the automated detection of both faces and hands, and we analyze the frequency of both faces and hands in the child’s visual environment. 

# Methods
## Participants
```{r echo=FALSE, include=FALSE}
all_subs <- read.csv("../data/demographics/all_participants.csv") %>%
  rename(subid = subID)
inc_subs <- read.csv("../data/demographics/demographics.csv")
all_subs$inc <- all_subs$reason=="included"

inc_subs$len.min <- sapply(strsplit(as.character(inc_subs$len),":"),
       function(x) {
         x <- as.numeric(x)
         x[1]+x[2]/60
       }
)

all_subs %>% 
  group_by(reason) %>%
  summarise(n = n()) %>% 
  kable

all_subs %>%
  left_join(inc_subs) %>%
  group_by(age_group) %>%
  summarise(n = sum(inc), 
            percent_included = mean(inc),
            age = mean(age.at.test, na.rm=TRUE), 
            video_length = mean(len.min, na.rm=TRUE), 
            female = sum(gender=="female", na.rm=TRUE)) %>%
  xtable %>%
  print(digits = 2, include.rownames=FALSE,table.placement = "H", type="latex", comment = F)
```

Our final sample consisted of 36 infants and children, with 8 participants in three age groups: 8 months (6 females), 12 months (7 females), and 16 months (6 females). Participants were recruited from the surrounding community via state birth records, had no documented disabilities, and were reported to hear at least 80\% English at home. Demographics and exclusion rates are given in Table \ref{tab:pop}. 
\begin{table}[H]
\centering
\begin{tabular}{rrrrrr}
  \hline
 Group & N & \% incl. & Mean age & Videos length (min) \\ 
  \hline
   8 mo. &   12 & 0.46 & 8.71 & 14.41 \\ 
   12 mo. &  12 & 0.40 & 12.62 & 13.48 \\ 
   16 mo. &  12 & 0.31 & 16.29 & 15.00\\ 
   \hline
\end{tabular}
\caption{\label{tab:pop} Demographics by age group.}
\end{table}
To obtain this final sample, we tested `r length(all_subs$reason)` children, excluding  `r sum(all_subs$reason!="included")` children for the following reasons:
`r sum(all_subs$reason=="HC technical error")` for technical issues related to the headcam,
`r sum(all_subs$reason=="would not wear camera")` for failing to wear the headcam,
`r sum(all_subs$reason=="less than 4 min HC footage")` for fewer than 4 minutes of headcam footage,
`r sum(all_subs$reason=="multiple people")` for having multiple adults present,
`r sum(all_subs$reason=="no CDI")` for missing CDI data,
`r sum(all_subs$reason=="no DV cam footage")` for missing scene camera footage,
`r sum(all_subs$reason=="infant crying")` for fussiness, and one excluded for sample symmetry. All inclusion decisions were made independent of the results of subsequent analyses.

## Head-mounted camera
```{r image, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=2, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.label="headcam", fig.cap = "Field of view for three different headcam configurations, with the device we used in the middle. The lowest camera is pictured for comparison, but was not available until after our study was already in progress."}
img <- png::readPNG("images/viewangle.png")
grid::grid.raster(img)
```

We used a small, head-mounted camera ("headcam") that was constructed from a MD80 model camera attached to a soft elastic headband. Videos captured by the headcam were 720x480 pixels with 25 frames per second.Detailed instructions for creating this headcam can be found  at \url{http://babieslearninglanguage.blogspot.com/2013/10/how-to-make-babycam.html}. A fisheye lens was attached to the camera to cincrease the view angle from $32^{\circ}$ horizontal by $24^{\circ}$ vertical to $64^{\circ}$ horizontal by $46^{\circ}$ vertical (see Figure \ref{fig:headcam}, left). 

Even with the fish-eye lens, the vertical field of view of the camera is still considerably reduced compared to the child's approximate vertical field of view, which spans around 100--120$^{\circ}$ in the vertical dimension by 6-7 months of age [@mayer1988;@cummings1988]. As we were primarily interested in the presence of faces in the child's field of view, we chose to orient the camera upwards to capture the entirely of the child's upper visual field where the child is likely to see the faces of adults around them. This allowed us to maximize our chances of capturing faces that the child would have seen during the play session.

## Procedure

First, all parents signed consent documents in a waiting room  where children were fitted with the headcam. After the child was comfortable in the waiting room and with the experimenter, the experimenter placed the headcam on the child's head. If the child was uninterested in wearing the headcam or tried to take it off, the experimenter presented engaging toys to try to draw the child's focus away from the headcam [@yoshida2008].

After the child was comfortable with wearing the headcam, the child and caregiver were shown to a playroom for the free-play session--the focus of the current study. Parents were shown a box containing three pairs of novel and familiar objects (e.g., a ball and a feather duster, named a "zem"), and were instructed to play with the object pairs with their child one at a time, "as they typically would." All parents confirmed that their child had not previosuly seen the novel toys and were instructed to use the novel labels to refer to the novel toys.  

The experimenter then left the playroom for approximately 15 minutes, during which a tripod-mounted camera in the corner of the room recorded the session and the headcam captured video from the child's perspective.

## Data Processing and Annotation

\begin{figure*}
\includegraphics[width=6in]{images/framesample.pdf}
\caption{\label{fig:frames} Example face detections made by MTCNN for the headcam videos from a child in each group  (green squares).}
\end{figure*}

Headcam videos were trimmed such that they excluded the instruction phase when the experimenter was in the room and were automatically synchronized with the tripod-mounted videos using FinalCut Pro Software. These sessions yielded videos of 
`r round(sum(inc_subs$len.min), digits = 0)` minutes (almost a milion frames), with an average video length of
`r round(sum(inc_subs$len.min)/60, digits = 2)` minutes
(min = `r round(min(inc_subs$len.min), digits = 2)`, max = `r round(max(inc_subs$len.min), digits = 2)`).

### Posture and Orientation Annotation
We created a set of custom annotations that described the child's physical posture (e.g. standing) and the orientation of the caregiver relative to the child (e.g. far away). The child's posture was categorized as being held/carried, prone (crawling or lying), sitting, or standing. The caregiver's orientation was characterized as being close to the child, farther from the child, and a global category of caregiver behind the child. For the first two annotationes (close/far from the child), the caregiver could either be to the the front or to the side of the child. All annontations were made using OpenSHAPA/Datavyu software  [@adolph2012], and times when the child was out of view of the tripod camera was marked as uncodable and was excluded from these annotations. 

## Face Detection
Measuring the availability of caregiver’s faces across development was an important goal of the study. We thus used a total of three face detection systems to explore this hypothesis and to avoid the cost of hand-annotating every frame. We first measure the performance of the most commonly-used and widely avlaialbe face detection algorithmsm (Viola-Jones). We use this as a benchmark for performance, as while it can achieve impressive accuracy in some situations, it is is notoriously bad at dealing with occluded faces (@scheirer2014perceptual). We next capitalized on recent improvements in computer vision, testing the performance of two other algorithms that extract face information in distinct ways.  Rsults for the evaluation of these three systems are shown in Table XX.

The second of these algorithms (OpenPose) also captures information about the "skeleton" of people in a given frame, outputting information about the entire set of body parts that were detected. This allowed us to extract hand information as well as wrist information. We analyze wrist detections as a good proxy for the presence of a hand. We do so first because we did not capture the entire visual field experienced by the infants/children. Thus evne though the video may only show the presence of a wrist, the caregiver's hand may be within the child's field of view. Second, we do so because hands are often occluded by objects when caregivers are interacting with children, yet still visually accessible by the child and part of their joint interaction. For example,if a caregiver was holding a toy and presenting it to the infant, their wrists may be visible but their hand may not necessarily be (as it is occluded by the toy). 

### Face Detection Algorithms
The first face detection system made use of a series of Haar feature-based cascade classifiers (Viola-Jones, @viola2004robust) applied to each individual frame. This detector provided information about the presence of a face as well as its size and position. 

The second algorithm was based on the work by @zhang2016 using multi-task cascaded convolutional neural networks (MTCNNs). The system was built using a novel cascaded CNN-based framework for joint detection and alignment, built to perform well in real-world environments where varying illuminations and occlusions are present. We used a Tensorflow implementation of this algorithm provided by (https://github.com/davidsandberg/facenet). Like Viola-Jones, this detector provided information about the presence of a face as well as it's size and position.

The third algorithm was a CNN-based pose detector that provided the locations of 18 body parts (ears, nose, wrists, etc) (@cao2017realtime, @simon2017hand, @wei2016cpm) avaliable at https://github.com/CMU-Perceptual-Computing-Lab/openpose. The system uses a CNN for initial anatomical detection and subsequently applies part affinity fields (PAFs) for part association, producing a series of body part candidates. The candidates are then matched to a single individual and finally assembled into a pose. For the purposes of our investigation we only made use of the body parts relevant to the face (ears, eyes, nose). We operationalized face detections as any frames containing a nose and either the left or right eye; we operationalized hand detections as any frames containing any of the 21 points detected on either the left or the right hand, and we operationalized wrist detections any frames containing either the left or the right wrist.  

### Detector evaluation
In order to evaluate the performance of these detectors, we constructed a "gold set" of frames
Evaluating face detectors (text here about how gold standard was selected)

```{r, message=FALSE, warning=FALSE, results='asis'}
data_mtcnn <- read_csv("../data/final_output/mtcnn3.csv") %>%
  distinct(video, frame, .keep_all = TRUE)

data_openpose <- read_csv("../data/final_output/openpose_results_truncated_2.csv") %>%
  distinct(name, frame, .keep_all = TRUE) %>%
  mutate(frame = sprintf("%05d", as.numeric(frame) + 1)) %>%
  mutate(is_face = ifelse(Nose_conf != 0, "True", "False")) %>%
  select(group, name, frame, is_face) %>%
  rename(video = name)

original_candidates <- read_csv("../data/ground_truth/gold_set_candidates2.csv") %>%
  arrange(video, frame)

ground_truth_data <- read_csv("../data/ground_truth/ground_truth3.csv") %>%
  select(-angle) %>%
  arrange(video, frame) %>%
  cbind(original_candidates %>%
          select(sample_type))

evaluate <- function(a, b) {
  if (a == "True") {
    if (a == b) return ("TP")
    else return("FN")
  }
  else {
    if (a == b) return("TN")
    else return("FP")
  }
}

mtcnn_results <- data_mtcnn %>%
  rename(detector_is_face = is_face) %>%
  inner_join(ground_truth_data, by = c("group", "video", "frame")) %>%
  rowwise() %>%
  mutate(result = evaluate(is_face, detector_is_face))

openpose_results <- data_openpose %>%
  rename(detector_is_face = is_face) %>%
  inner_join(ground_truth_data, by = c("group", "video", "frame")) %>%
  rowwise() %>%
  mutate(result = evaluate(is_face, detector_is_face))

get_prf <- function(df) {
  tp <- sum(df$result == "TP")
  fp <- sum(df$result == "FP")
  fn <- sum(df$result == "FN")
  
  p <- tp / (tp + fp)
  r <- tp / (tp + fn)
  f <- 2 * p * r / (p + r)
  return(c(p, r, f))
}

render_table <- function(df, detector) {
  hd <- df %>%
    filter(sample_type == "high_density")
  
  prf_vec <- get_prf(hd)
  
  xtable::xtable(tibble(P = prf_vec[1], R = prf_vec[2], F = prf_vec[3]), caption=paste(detector, "High Density Sample")) %>%
    print(type="latex", comment = F, table.placement = "H")
  
  r <- df %>%
    filter(sample_type == "random")
  
  prf_vec <- get_prf(r)
  
  xtable::xtable(tibble(P = prf_vec[1], R = prf_vec[2], F = prf_vec[3]), caption=paste(detector, "Random Sample")) %>%
    print(type="latex", comment = F, table.placement = "H")
}

render_table(mtcnn_results, "MTCNN")
render_table(openpose_results, "OpenPose")

```

```{r xtable, eval=FALSE, include=FALSE, results="asis"}
  tab3 <- xtable::xtable(summary(fullModelminsInt_Handd)$coef, digits=3,
                        caption = "Results from GLMM model prediting the presence/absence of hands (OpenPose-Hands) across the entire dataset.")

  print(tab3, type="latex", comment = F, table.placement = "H")
```


Evaluating hand/wrist detectors (text here about how gold standard was selected)

\begin{table}[H]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & z value & Pr($>$$|$z$|$) \\ 
  \hline
(Intercept) & -4.846 & 0.077 & -62.682 & 0.000 \\ 
  Age & 0.170 & 0.061 & 2.766 & 0.006 \\ 
  Posture-Prone & -0.513 & 0.036 & -14.045 & 0.000 \\ 
  Posture-Sit & 0.648 & 0.034 & 18.853 & 0.000 \\ 
  Posture-Stand & 1.005 & 0.035 & 28.402 & 0.000 \\ 
  Orient.-Close & 1.549 & 0.021 & 72.757 & 0.000 \\ 
  Orient.-Far & 2.258 & 0.022 & 101.138 & 0.000 \\ 
   \hline
\end{tabular}
\caption{TABLE PLACEHOLDER} 
\end{table}



# Results
First, we report developmental shifts in infants' posture and their orientation to their caregiver. Then, we explore how these changes influence children's visual access to faces and to hands. Finally, we explore how these changes impact the accessibility of faces and hands during labeling events.

```{r echo=FALSE, include=FALSE}
d <- read_csv("../data/consolidateddata/consolidated_data_4dets.csv") %>%
  mutate(posture = factor(posture), orientation = factor(orientation))

## make long form version for plotting both face detections on same graph
dlong <- d %>%
  mutate(id = paste(subid,frame)) %>%
  gather(key = detectorType, value = detection, faceMT, faceOP, handOP, wristOP) %>%
  mutate(detectorType=as.factor(detectorType))  

# for better plotting names
#levels(dlong$detectorType) 
#[1] "faceMT"  "faceOP"  "handOP"  "wristOP"
levels(dlong$detectorType) <- c("MTCNN-Faces", "OpenPose-Faces","OpenPose-Hands", "OpenPose-Wrists") 
```

## Changes in Posture and Orientation


```{r echo=FALSE, include=FALSE}
orientation <- d %>%
  filter(!is.na(orientation)) %>%
  group_by(age.grp, subid, orientation) %>%
  summarise(time = sum(dt, na.rm=TRUE)) %>%
  mutate(prop.time = time/sum(time)) %>%
  group_by(subid, age.grp, orientation) 

posture <- d %>%
  filter(!is.na(posture)) %>%
  group_by(age.grp, subid, posture) %>%
  summarise(time = sum(dt, na.rm=TRUE)) %>%
  mutate(prop.time = time/sum(time)) %>%
  group_by(subid, age.grp, posture) 
```

```{r orientation, fig.env="figure", fig.pos = "h", fig.align = "center", fig.width=3, fig.height=2, fig.cap = "Proportion time that infants in each age group spent in each posture (left panel) and orientation relative to their caregiver (right  panel)." }

ggplot(orientation, aes(x = factor(age.grp), y = prop.time, col = orientation)) + 
  geom_boxplot() + 
  ylab("Proportion Time") + 
  xlab("Age (months)") + 
  theme_bw()
```
We noted characteristic changes in infants posture and orientation across this developmental time range. The proportion of time infant's spent sitting decreased with age, and the proportion of time infant's spent standing increased with age. Both 8-month-olds and 12-month-olds psent equivalent amounts of time either lying/crawling, which was markedly decreased in the 16-month-olds, who spent most of their time either sitting or standing (see Figure \ref{fig:posture}).  We also observed characteristic changes in children's orientation relative to their caregivers: the 8-month-olds spent more time with their caregiver behind them supporting their sitting positions (see Figure \ref{fig:orientation}). 

```{r posture, fig.env="figure", fig.pos = "h", fig.align = "center", fig.width=3, fig.height=2, fig.cap = "Proportion time that infants in each age group spent in each posture (left panel) and orientation relative to their caregiver (right  panel)." }
ggplot(posture, aes(x = factor(age.grp), y = prop.time, col = posture)) + 
  geom_boxplot() + 
  ylab("Proportion Time") + 
  xlab("Age (months)") + 
  theme_bw()
```

# Changes in Access to Faces & Hands

We first examined the proportion of face and hand detections across age; a full summary can be seen in Figure \ref{fig:detsbyAge}. We observed  a slight u-shaped function both when analyzing the output of the MTCNN and OpenPose detectors, such that 12-month-olds may experience less faces than 8 or 12-month-olds. Next, we observed that hand detections increased with the age of the participant, though they were relatively infrequent in this dataset. As expected, we found that wrist detections we relatively more frequent and also appeared to increase somewhat with age, suggesting that caregivers hands may be just outside of the field of view of our camera or that they were often occluded by toys.
 
Next, we turned to our hypothesis of interest, examinig the impact of postural and locomotive developments on children's visual access to faces and hands. Overall, we ofund that children's posture was a major factor both in how many faces they saw during the play session. Infants who were sitting saw more faces than infants who were lying down or being carried, while infants who were standing saw the most faces. We observed a similar pattern with respect to hands: children who were sitting or standing tended to see more hands/wrists than children who were lying down or held. 

We next examined how the child's orientation relative to their caregiver impacted their visual access to faces and hands. Overlal, we saw that children who were far away from their caregiver were more likely to see faces and hands than children who were close to their caregiver; this was true within all age groups and for all detectors. 

```{r detByAge, fig.env="figure*", fig.pos = "h", fig.align = "center", fig.width=7, fig.height=6, fig.cap = "Proportion face (left) and hand (right) detections as a function of participant's age."  }
detectionsByAge <- dlong %>%
  filter(!is.na(age.at.test)) %>%
  group_by(subid, age.at.test, detectorType) %>%
  summarise(detections = mean(detection)) %>%
  left_join(inc_subs) %>%
  mutate(len = as.numeric(len.min))

p1=ggplot(detectionsByAge, aes(x = age.at.test, y = detections, color = detectorType, size=len.min)) +
    geom_jitter(width = .25) +
    geom_smooth(size=1) +
    scale_x_continuous(breaks=c(8,12,16)) +
    scale_size_continuous(range=c(.01,2),name = "Video Length (mins)") +
    labs(y = "Prop. detected", x = element_blank()) +
    theme_bw() + 
    theme(legend.position="none") +
    scale_color_manual(values=c("#1F618D", "#7FB3D5", "#C70039", "#FF5733")) +
    facet_grid(~detectorType, scales = "free_y") 

detectionsByPosture <- dlong %>%
  filter(!is.na(posture)) %>%
  group_by(age.grp, posture, subid,detectorType) %>%
  summarise(face = mean(detection)) %>%
  group_by(age.grp)

p2=ggplot(detectionsByPosture, aes(x = factor(age.grp), y = face, col = posture)) + 
  geom_boxplot(outlier.size = 0.5) + 
  labs(y = "Prop. detected", x = element_blank()) +
  facet_grid(~detectorType, scales="free_y") +
  theme_bw() +
  theme(legend.position="top", legend.text=element_text(size = 8), legend.margin=margin(t=0, r=0, b=0, l=0, unit="cm"))


detectionsByOrientation <- dlong %>%
  filter(!is.na(orientation)) %>%
  group_by(age.grp, orientation, subid,detectorType) %>%
  summarise(detections = mean(detection)) %>%
  group_by(age.grp)

p3=ggplot(detectionsByOrientation, aes(x = factor(age.grp), y = detections, col = orientation)) + 
  geom_boxplot(outlier.size = 0.5) + 
  labs(y = "Prop. detected", x = element_blank()) +
  facet_grid(~detectorType, scales="free_y") +
  theme_bw() +
  theme(legend.position="top", legend.text=element_text(size = 8), legend.margin=margin(t=0, r=0, b=0, l=0, unit="cm")) +
  labs(y = "Prop. detected", x = element_blank()) 
  
grid.arrange(p1, p2, p3, nrow=3)

```

## Model

<!-- ```{r echo=FALSE, include=FALSE} -->
<!-- d$faceNum=as.numeric(d$faceMT) -->
<!-- d$handNum=as.numeric(d$handOP) -->

<!-- # Fails to converge -->
<!-- # fullModel <- glmer(is_face ~ scale(age.at.test)*(posture) *(orientation) + (1|subid), -->
<!-- #       data = d, -->
<!-- #       family = "binomial") -->

<!-- # Fails to converge -->
<!-- # agexOrientation <- glmer(faceNum ~ scale(age.at.test) * orientation + (1|subid), -->
<!-- #       data = d, -->
<!-- #       family = "binomial") -->

<!-- fullModelminsInt_Faces <- glmer(faceNum ~ scale(age.at.test) + posture + orientation + (1|subid), -->
<!--        data = d, -->
<!--        family = "binomial") -->

<!-- fullModelminsInt_Handd <- glmer(handNum ~ scale(age.at.test) + posture + orientation + (1|subid), -->
<!--        data = d, -->
<!--        family = "binomial") -->

<!-- ageOnly <- glmer(faceNum ~ scale(age.at.test) + (1|subid), -->
<!--       data = d, -->
<!--       family = "binomial") -->

<!-- ``` -->

For our anlaysis, we fit models to the two models that the had the highest F-scores (i.e., highest performance on detecting faces and hands, respectively). To formalize these observations, we fit a generalized logistic mixed-effect model to all with the presence/absence of a face on every frame as the dependent variable, and participant's age, orientation, and posture as predictors. Interactions between predictors were not included as this maximal model failed to converge. A summary of the coefficients of this model can be found in Table XX.  While age remained a significant predictor even when accounting for the effects of infants' posture and caregiver's oirentation, it held relativley less predictive power. Overall, these results confirm that infant's access to faces is heavily infleunced by their own posture and their caregivers orientation towards them.

<!-- ```{r xtable, results="asis"} -->
<!-- tab2 <- xtable::xtable(summary(fullModelminsInt_Faces)$coef, digits=3, -->
<!--                       caption = "Results from GLMM model prediting the presence/absence of a face (MTCNN-Faces) across the entire dataset.") -->

<!-- print(tab2, type="latex", comment = F, table.placement = "H") -->
<!-- ``` -->

<!-- ```{r xtable, results="asis"} -->
<!--  tab3 <- xtable::xtable(summary(fullModelminsInt_Handd)$coef, digits=3, -->
<!--                        caption = "Results from GLMM model prediting the presence/absence of hands (OpenPose-Hands) across the entire dataset.") -->

<!--  print(tab3, type="latex", comment = F, table.placement = "H") -->
<!-- ``` -->

\begin{table}[H]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & z value & Pr($>$$|$z$|$) \\ 
  \hline
(Intercept) & -4.846 & 0.077 & -62.682 & 0.000 \\ 
  Age & 0.170 & 0.061 & 2.766 & 0.006 \\ 
  Posture-Prone & -0.513 & 0.036 & -14.045 & 0.000 \\ 
  Posture-Sit & 0.648 & 0.034 & 18.853 & 0.000 \\ 
  Posture-Stand & 1.005 & 0.035 & 28.402 & 0.000 \\ 
  Orient.-Close & 1.549 & 0.021 & 72.757 & 0.000 \\ 
  Orient.-Far & 2.258 & 0.022 & 101.138 & 0.000 \\ 
   \hline
\end{tabular}
\caption{Results from GLMM model prediting the presence/absence of a face across the entire dataset (MTCNN detectors).} 
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & z value & Pr($>$$|$z$|$) \\ 
  \hline
(Intercept) & -8.775 & 0.236 & -37.259 & 0.000 \\ 
  Age & 0.879 & 0.166 & 5.292 & 0.000 \\ 
  Posture-Prone  & 1.215 & 0.110 & 11.082 & 0.000 \\ 
  Posture-Sit & 1.943 & 0.108 & 18.000 & 0.000 \\ 
  Posture-Stand  & 2.175 & 0.107 & 20.260 & 0.000 \\ 
  Orient.-Close  & 0.608 & 0.059 & 10.371 & 0.000 \\ 
  Orient.-Far & 2.244 & 0.060 & 37.222 & 0.000 \\ 
   \hline
\end{tabular}
\caption{Results from GLMM model prediting the presence/absence of hands (OpenPose-Hands) across the entire dataset.} 
\end{table}


# General Discussion
We use a head-mounted camera to explore how children's postural and locomotive development directly impacts their access to social information relevant for word-learning, here operationalized as the presence of the faces and hands of their caregiver. We found that children's posture and orientation towards their caregiver changed systematically across age, and that all three of these factors dramatically impacted the proportion of faces and hands that were avaliable in the child's visual field. Thus, infants postural development is mediating factor that explains age-related changes in the proportion of faces and hands avaliable in infants' visual field.

This work also deploys novel advancements in computer vision to the study of developmental psychology. The field of object detection and recognition has advanced dramatically in the past five years since the re-birth of deep learning algorithms @krizhevsky2012imagenet, creating a new generation of algorithmic tools. These tools are both substantially better equipped to deal with noisier, more complicated datasets and that can extract richer and more detailed information. Videos from the infant perspective provided sustantial challenges (e.g., partially occluded faces) for the classc models of face detedction (e.g., ViolaJones, @viola2004robust).   urther, as the headcam technologies employed here were inexpensive (~$60 a camera) and the computer vision algorithms freely avaliable, this method is a promising avenue for quanitfying the visual and social information avaliable to infant learners. 

Thus, we suggest that the combined use of these new tools can be leveraged to understand the changing infant perspecive on the visual world and the implications of these changes for both linguistic, cognitive, and social development.

# Acknowledgements

Thanks to Kaia Simmons, Kathy Woo, Aditi Maliwal, and other members of the Language and Cognition Lab for help in recruitment, data collection, and annotation. This research was supported by a John Merck Scholars grant to MCF. An earlier version of this work was presented to the Cognitive Science Society in @frank2013. Please address correspondence to Michael C. Frank, Department of Psychology, Stanford University, 450 Serra Mall (Jordan Hall), Stanford, CA, 94305, tel: (650) 724-4003, email: \texttt{mcfrank@stanford.edu}.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
