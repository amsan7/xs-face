---
title             : "Postural developments modulate children's visual access to social information"
shorttitle        : "Posture modulates social access"

author: 
  - name          : "Bria L. Long"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Stanford CA 94305"
    email         : "bria@stanford.edu"
  - name          : "Alessandro Sanchez"
    affiliation   : "1"
  - name          : "Allison M. Kraus"
    affiliation   : "1"
  - name          : "Ketan Agrawal"
    affiliation   : "1"
  - name          : "Michael C. Frank"
    affiliation   : "1"
    

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"


# authornote: |
#   Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.
# 
#   Enter author note here.

abstract: |
  The ability to process social information is a critical component of children's early linguistic and cognitive development.
  However, as children reach their first birthday, they begin to locomote themselves, dramatically changing the way they see the world around them. 
  How do these postural and locomotor developments affect children's access to the social information relevant for word-learning? 
  We explored this question by using head-mounted cameras to record the egocentric visual perspective of 36 infants (at 8, 12, and 16 months of age) during a naturalistic play session with their caregiver. To estimate the proportion of faces and hands in the infant view, we used a computer vision algorithm to detect the presence of faces and hands in the entire dataset.
  We found that infants\' posture and orientation to their caregiver changed dramatically across this age range and modulated their access to this social information; infants who were sitting or standing with their caregiver at a close distance tended to have the most faces/hands in their visual field. 
  We also applied our automated analysis to the egocentric video data from a recent paper documenting how posture changes one-year-olds visual access to faces (Franchak et al., 2017), finding convergence across both methodologies and dataset and confirming previous work that motoric developments play a significant role in the emergence of children’s linguistic and social capacities.
  We suggest that the combined use of head-mounted cameras and the application of new computer vision techniques is a promising avenue for understanding the statistics of infants\' visual and linguistic experience as they change over development.

  
keywords          : "Postural developments modulate children's visual access to social information"
wordcount         : "4295"

bibliography      : ["xsface.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---


```{r, libraries}
library(knitr)
knitr::opts_chunk$set(fig.width=6, fig.height=5, fig.crop = FALSE, 
                      fig.path='figs/', echo=FALSE, warning = FALSE, 
                      cache=TRUE, message = FALSE, sanitize = TRUE)
library(readr)
library(dplyr)
library(tidyr)
library(xtable)
library(lubridate)
library(langcog)
library(ggplot2)
library(ggthemes)
library(lme4)
library(gridExtra)
library(assertthat)
library(tidyverse)
library(viridis)
library(papaja)
library(png)
library(magick)

# base_size in theme_few (changes font size in plots)
plot_size=16
```

From their earliest months, infants are deeply engaged in learning from others.  Even newborns tend to prefer to look at faces with direct vs. averted gaze [@farroni2002eye] and young infants follow overt gaze shifts. And as infants learn to parse and segment their native language(s), the social cues provided by speakers (e.g., eye-gaze) may provide strong scaffolding for early word learning. Indeed, empirical work suggests that children’s ability to process social cues is a key factor in early language development: in a longitudinal study, children’s level of joint engagement with their mother at 9-12 months was found to predict both their receptive and productive vocabularies [@carpenter1998]. More specifically, 10 month-olds who follow an adult’s gaze in an experimental context have larger vocabularies at 18 months and throughout the second year of life [@brooks2005; @brooks2008]. 

Around their first birthday, however, children’s ability to interact with their world radically changes [@adolph2006motor] as they are no longer constrained to the same spot that their caregivers last placed them in. Before the first birthday, children often begin crawling; soon after, they begin to walk independently. While not all children crawl [@adolph1998roles]—or crawl in the same way—children tend to find creative ways of moving (e.g., scooting) or getting others to help them move (e.g cruising) to begin to explore their world on their own [@patrick2012developmental]. As independent agents in the world, children are more active in the construction of their own learning environments--yet spend much of their time in a world primarily populated by knees

One possibility is that these motor improvements have strong developmental cascades, impacting children’s emerging social, cognitive, and linguistic abilities [@iverson2010]. Indeed, these postural changes change how children interact with their mothers; walking infants make different kinds of object-related bids for attention from their mothers than crawling infants, and tend to hear more action directed statements (e.g., “open it”) [@karasik2014]. In an observational study, @walle2014 also found that children who were able to walk had both higher receptive and productive vocabularies. On their account, children’s ability to stand and independently locomote may fundamentally change their ability to access the social information (e.g., facial expressions, gaze cues, pointing) relative to children who are still crawling and sitting. In other words, the ability of walking infants to access more detailed social information may allow infants to learn words quicker and more efficiently, facilitating language growth.

Over the past decade, researchers have started to use egocentric, head-mounted cameras to document the social information that infants and children have access to across early development, and to understand the degree to which there are substantial shifts in their viewpoints that may have downstream developmental consequences [@yoshida2008]. Indeed, the infant visual perspective has proved to be both remarkably different than the adult perspective and not easily predicted by our own intuitions [@franchak2011, @yoshida2008, @clerkin2017]. Over the first two years of life, the infant viewpoint seems to transition from primarily containing close up view of faces to capturing relatively restricted views of hands paired with the objects they are acting on [@fausey2016]; both computational and empirical work suggest that this restricted viewpoint may be more effective for learning about objects and their labels than the comparable adult perspective [@yurovsky2012, @bambach2017].

We hypothesized that children’s postural developments may be partially responsible for some of these changes in the infant perspective. During spontaneous play, toddlers are more likely to look at the floor while crawling than while walking (Franchak et al., 2011), when they have full visual access to their environment and the people in it (Kretch, Franchak, and Adolph, 2014). More recently, when 12-month-olds [@franchak2017see] participated in a free-play session with their caregivers, their own posture and as well as their caregiver’s posture influenced the proportion of time they spent looking at the faces and bodies of their caregivers [@franchak2017see].

Here, we directly examine the role that postural developments that infants’ experience as they reach their first birthday change the social information in the infant view. We expand on previous findings in three key ways. First, using head-mounted cameras during a brief laboratory free-play session, we record the visual experience of three groups of children at 8, 12, and 16 months-of-age, covering a broad range of ages and locomotor abilities around the first birthday; Children’s posture and orientation relative to their caregiver were also recorded from a third-person perspective and hand-annotated. This cross-sectional design thus allows us to directly examine the relative contributions of age vs. postural developments on children’s visual access to social information. Second, we use a modern computer vision algorithm for the automated detection of faces and hands in the egocentric viewpoints of infants.  In particular, we capitalize on recent improvements in face and pose detection algorithms [@zhang2016; @cao2017realtime] to analyze the frequencies of faces and hands in the child’s visual environment, both overall and relative to naming events by their caregivers. Use of this emerging technology allows us to annotate the entirety of the dataset, allowing a more complete picture of the changing infant perspective. We apply this same automated method to the dataset used in @franchak2017see, validating the generalizability of this technique. Third, as parents were presented with pairs of novel and familiar objects with their children during the play sessions, we were able to explore the availability of social signals during naming events.

Thus, the current study allows us to analyze changes in the visual access to faces and hands according to children's age, posture, and linguistic input. Broadly, we predicted that there would be differential access to social information based on children's postural developments: crawling infants would see fewer faces/hands because they would primarily be looking at the ground, while walking toddlers would have access to a richer visual landscape with greater access to the social information in their environment.

<!-- Second, we predicted that there would be a greater proportion of faces/hands in view around naming events—-broadly construed as any time a parent mentioned a label for one of the objects in the play set -->

# Methods
## Participants
```{r echo=FALSE, include=FALSE, demographics}
all_subs <- read.csv("../data/demographics/all_participants.csv") %>%
  rename(subid = subID)
inc_subs <- read.csv("../data/demographics/demographics.csv")
all_subs$inc <- all_subs$reason=="included"

inc_subs$len.min <- sapply(strsplit(as.character(inc_subs$len),":"),
                           function(x) {
                             x <- as.numeric(x)
                             x[1]+x[2]/60
                           }
)

all_subs %>% 
  group_by(reason) %>%
  summarise(n = n()) %>% 
  kable

all_subs %>%
  left_join(inc_subs) %>%
  filter(inc==TRUE) %>%
  group_by(age_group) %>%
  summarise(n = sum(inc),
            percent_included = mean(inc),
            age = mean(age.at.test, na.rm=TRUE),
            video_length = mean(len.min, na.rm=TRUE),
            female = sum(gender=="female", na.rm=TRUE)) 
  # xtable %>%
  # print(digits = 2, include.rownames=FALSE,table.placement = "H", type="latex", comment = F)

# <!-- \caption{\label{tab:pop} Demographics by age group.} -->
```

 Our final sample consisted of 36 infants and children, with 12 participants in three age groups: 8 months (6 F), 12 months (7 F), and 16 months (6 F). Participants were recruited from the surrounding community via state birth records, had no documented disabilities, and were reported to hear at least 80 percent English at home. Demographics and exclusion rates are given in the table below.

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
\hline
Group & N & \% incl. & Avg age & Avg video length (min) \\
\hline
8 mo. &   12 & 0.46 & 8.71 & 14.41 \\
12 mo. &  12 & 0.40 & 12.62 & 12.71 \\
16 mo. &  12 & 0.31 & 16.29 & 15.10\\
\hline
\end{tabular}
\end{table}

To obtain this final sample, we tested `r length(all_subs$reason)` children, excluding `r sum(all_subs$reason!="included")` children for the following reasons:
`r sum(all_subs$reason=="HC technical error")` for technical issues related to the headcam,
`r sum(all_subs$reason=="would not wear camera")` for failing to wear the headcam,
`r sum(all_subs$reason=="less than 4 min HC footage")` for fewer than 4 minutes of headcam footage,
`r sum(all_subs$reason=="multiple people")` for having multiple adults present,
`r sum(all_subs$reason=="no CDI")` for missing Communicative Development Inventory (CDI) data,
`r sum(all_subs$reason=="no DV cam footage")` for missing scene camera footage,
`r sum(all_subs$reason=="infant crying")` for fussiness, and one for sample symmetry. All inclusion decisions were made independent of the results of subsequent analyses. These data were also analyzed in @frank2013 and @sanchez2018postural.


## Head-mounted camera
```{r headcam, fig.env = "figure", fig.height=3, fig.pos = "H", fig.label="headcam", fig.cap = "Vertical field of view for two different headcam configurations (we used the lower in our current study)."}

img <- png::readPNG("images/viewangle_cut.png")
grid::grid.raster(img)
```

We used a small, head-mounted camera ("headcam") that was constructed from a MD80 model camera attached to a soft elastic headband. Videos captured by the headcam were 720x480 pixels with 25 frames per second [^2]. A fisheye lens was attached to the camera to increase the view angle from $32^{\circ}$ horizontal by $24^{\circ}$ vertical to $64^{\circ}$ horizontal by $46^{\circ}$ vertical (see Figure \ref{fig:headcam}, above). 

[^2]: Detailed instructions for creating this headcam can be found  at \url{http://babieslearninglanguage.blogspot.com/2013/10/how-to-make-babycam.html}.

Even with the fish-eye lens, the vertical field of view of the camera is still considerably reduced compared to the child's field of view, which spans around 100--120$^{\circ}$ in the vertical dimension by 6-7 months of age [@mayer1988;@cummings1988]. As we were primarily interested in the presence of faces in the child's field of view, we chose to orient the camera upwards to capture the entirety of the child's upper visual field where the child is likely to see adult faces, understanding that this decision limited our ability to detect hands (especially those of the child, which are typically found at the bottom of the visual field).


## Procedure
All parents signed consent documents while children were fitted with the headcam. If the child was uninterested in wearing the headcam or tried to take it off, the experimenter presented engaging toys to try to draw the child's focus away from the headcam. When the child was comfortable wearing the headcam, the child and caregiver were shown to a playroom for the free-play session. Parents were shown a box containing three pairs of novel and familiar objects (e.g., a ball and a microfiber duster, named a "zem"), and were instructed to play with the object pairs with their child one at a time, "as they typically would." All parents confirmed that their child had not previously seen the novel toys and were instructed to use the novel labels to refer to the toys. The experimenter then left the playroom for approximately 15 minutes, during which a tripod-mounted camera in the corner of the room recorded the session and the headcam captured video from the child's perspective.

## Data Processing and Annotation
Headcam videos were trimmed such that they excluded the instruction phase when the experimenter was in the room and were automatically synchronized with the tripod-mounted videos using FinalCut Pro Software. These sessions yielded 
`r round(sum(inc_subs$len.min), digits = 0)` minutes (almost a million frames) of video, with an average video length of
`r round(mean(inc_subs$len.min), digits = 2)` minutes
(min = `r round(min(inc_subs$len.min), digits = 2)`, max = `r round(max(inc_subs$len.min), digits = 2)`).

### Posture and Orientation Annotation
We created custom annotations to describe the child's physical posture (e.g. standing) and the orientation of the child relative to the caregiver (e.g. far away). The child's posture was categorized as being held/carried, prone (crawling or lying), sitting, or standing. The caregiver's orientation was characterized as being close, far, or behind the child (independent of distance). For the first two annotations (close/far from the child), the caregiver could either be to the front or side of the child. All annotations were made by a trained coder using the OpenSHAPA/Datavyu software [@adolph2012toward]. Times when the child was out of view of the tripod camera were marked as uncodable and were excluded from these annotations. 

## Face and Hand Detection
We used three face detection systems to measure infants' access to faces. The first of these is the most commonly-used and widely available face detection algorithm: Viola-Jones. We used this algorithm as a benchmark for performance, as while it can achieve impressive accuracy in some situations, it is notoriously bad at dealing with occluded faces [@scheirer2014perceptual]. We next tested the performance of two face detectors that both made use of recently developed Convolutional Neural Networks (CNNs) to extract face information. The first algorithm was specifically optimized for face detection, and the second algorithm was optimized to extract information about the position of 18 different body parts. For the second algorithm [OpenPose; @cao2017realtime], we used the agent's nose (one of the body parts detected) to operationalize the presence of faces, as any half of a face necessarily contains a nose.

The OpenPose detector also provided us with the location of an agent's wrists, which we used as a proxy for hands for two reasons. First, as we did not capture children's entire visual field, the presence of a wrist is likely often indicative of the presence of a hand within the field of view. Second, hands are often occluded by objects when caregivers are interacting with children, yet still visually accessible by the child and part of their joint interaction. 


```{r frames, fig.env = "figure", fig.height=3, fig.pos = "H", fig.label="headcam", fig.cap = "Example face and pose detections made by OpenPose (top row) and MTCNN (bottom row) from a child in each age group. The last column features a false positive from OpenPose and a false negative from MTCNN."}

img <- png::readPNG("images/detector_samples_banner.png")
grid::grid.raster(img)
```



### Algorithms
The first face detection system made use of a series of Haar feature-based cascade classifiers [@viola2004robust] applied to each individual frame. The second algorithm (based on work by @zhang2016) uses multi-task cascaded convolutional neural networks (MTCNNs) for joint face detection and alignment, built to perform well in real-world environments where varying illuminations and occlusions are present. We used a Tensorflow implementation of this algorithm available at https://github.com/davidsandberg/facenet. 

The CNN-based pose detector  [OpenPose; @cao2017realtime; @simon2017hand; @wei2016cpm] provided the locations of 18 body parts (ears, nose, wrists, etc.) and is available at https://github.com/CMU-Perceptual-Computing-Lab/openpose. The system uses a CNN for initial anatomical detection and subsequently applies part affinity fields (PAFs) for part association, producing a series of body part candidates. The candidates are then matched to a single individual and finally assembled into a pose; here, we only made use of the body parts relevant to the face and hands (nose and wrists).  

### Detector evaluation
To evaluate face detector performance, we hand-labeled a "gold set" of labeled frames. To account for the relatively rare appearance of faces in the dataset, we hand-labeled two types of samples: a sample containing a high density of faces (half reported by MTCNN, half by OpenPose) and a random sample from the remaining frames. Each sample was comprised of an equal number of frames taken from each child's video. For wrist detections, the "gold set" was constructed in the same manner, except frames with a high density of wrists came only from detections made by OpenPose.  Faces were classified as present if at least half of the face was showing; wrists were classified as present if any part of the wrist was showing. Two authors labelled the frames independently and resolved disagreements on a case-by-case basis. Precision (hits / hits + false alarms), recall (hits / hits + misses), and F-score (harmonic mean of precision and recall) were calculated for all detectors and are reported in Table 1.


```{r loadDetections, message=FALSE, warning=FALSE, echo=FALSE, include=FALSE}
### READ IN DETECTIONS
d <- read_csv("../data/consolidated_data/consolidated_data_4detectors_march2020.csv") %>%
  mutate(posture = factor(posture), orientation = factor(orientation))

## make long form version for plotting both face detections on same graph
dlong <- d %>%
  mutate(id = paste(subid,frame)) %>%
  gather(key = detectorType, value = detection, faceMT, faceOP, faceVJ, wristOP) %>%
  mutate(detectorType=as.factor(detectorType)) 

# for better plotting names
levels(dlong$detectorType) 
# [1] "faceMT"  "faceOP"  "faceVJ"  "wristOP"
levels(dlong$detectorType) <- c("MTCNN-Faces", "OpenPose-Faces","ViolaJones-Faces","OpenPose-Wrists") 

```

```{r groundTruthSetup, message=FALSE, warning=FALSE, echo=FALSE, include=FALSE}
### READ IN GROUND TRUTH and load function
ground_truth_faces <- read_csv("../data/ground_truth/ground_truth_faces.csv") %>%
  mutate(sample_type=as.factor(sample_type)) 

ground_truth_wrists <- read_csv("../data/ground_truth/ground_truth_wrists.csv") %>%
  mutate(sample_type=as.factor(sample_type))

# Renaming for table output
levels(ground_truth_faces$sample_type)=c("High density", "Random")
levels(ground_truth_wrists$sample_type)=c("High density", "Random")

# Function to evaluate detectors
evaluate <- function(a, b) {
  if (a == TRUE) {
    if (a == b) return ("TP") # was face/wrist, detected face/wrist
    else return("FN") # was face/wrist, missed face/wrist
  }
  else {
    if (a == b) return("TN") # was not face/wrist, did not detect face/wrist
    else return("FP") # was not face/wrist, detected face/wrist
  }
}
```

```{r evaluateDetectors, include=FALSE, echo=FALSE}
## Evalute face and wrist detectors
performanceFaces <- ground_truth_faces  %>%
  mutate(frame=as.numeric(frame), subid=video) %>%
  left_join(dlong)   %>%
  filter(detectorType!="OpenPose-Wrists") %>%
  mutate(detector_is_face = detection, is_face = as.logical(is_face)) %>%
  rowwise() %>%
  mutate(result = evaluate(is_face, detector_is_face)) # always have the gronud truth first in evaluate argument

perfFacesbySampleType <- performanceFaces %>%
  group_by(detectorType, sample_type) %>%
  summarise(tp=sum(result == "TP"), fp=sum(result == "FP"), fn=sum(result=="FN"))  %>%
  group_by(detectorType, sample_type) %>%
  summarise(p = tp / (tp + fp), r = tp / (tp + fn), f=( 2 * p * r )/ (p + r)) 
  # xtable %>%
  # print(digits = 2, include.rownames=FALSE,table.placement = "H", type="latex", comment = F)

perfFacesOverall <- performanceFaces %>%
  group_by(detectorType) %>%
  summarise(tp=sum(result == "TP"), fp=sum(result == "FP"), fn=sum(result=="FN")) %>%
  group_by(detectorType) %>%
  summarise(p = tp / (tp + fp), r = tp / (tp + fn), f=( 2 * p * r )/ (p + r)) 

## EVALUATE WRIST DETECTOR
performanceWrists <- ground_truth_wrists  %>%
  mutate(frame=as.numeric(frame), subid=video) %>%
  left_join(dlong) %>%
  filter(detectorType=="OpenPose-Wrists") %>%
  mutate(detector_is_wrist = detection, is_wrist = as.logical(is_wrist)) %>% 
  rowwise() %>%
  mutate(result = evaluate(is_wrist, detector_is_wrist)) 

perfWristsbySampleType<- performanceWrists %>%
  group_by(detectorType, sample_type) %>%
  summarise(tp=sum(result == "TP"), fp=sum(result == "FP"), fn=sum(result=="FN")) %>%
  group_by(detectorType, sample_type) %>%
  summarise(p = tp / (tp + fp), r = tp / (tp + fn), f=( 2 * p * r )/ (p + r)) 
  xtable %>%
  print(digits = 2, include.rownames=FALSE,table.placement = "H", type="latex", comment = F)

perfWristsOverall<- performanceWrists %>%
  group_by(detectorType) %>%
  summarise(tp=sum(result == "TP"), fp=sum(result == "FP"), fn=sum(result=="FN")) %>%
  summarise(p = tp / (tp + fp), r = tp / (tp + fn), f=( 2 * p * r )/ (p + r)) 

## get rid of ViolaJones detections from data structure for future analyses
dlong <- dlong %>%
  filter(detectorType!="faceVJ") 

```


For face detection, MTCNN outperformed OpenPose when taking into account only the composite F-score (`r round(perfFacesOverall$f[1],2)` MTCNN vs. `r round(perfFacesOverall$f[2],2)` OpenPose).  MTCNN and OpenPose performed comparably with the random sample, with MTCNN having higher precision on the high density sample, suggesting that OpenPose generated slightly more false positives than MTCNN. ViolaJones performed quite poorly relative to the other detectors, especially with respect to the random sample. For wrist detection, OpenPose performed moderately well (F = `r round(perfWristsOverall$f,2)`) with relatively high precision but low recall on the randomly sampled frames (see Table 1). We thus analyze wrist detections, with the caveat that we are likely underestimating the proportion of hands in the dataset.


\begin{table}[ht]
\centering
\begin{tabular}{rllrrr}
\hline
Algorithm & Sample\ Type & P & R & F \\ 
\hline
MTCNN-Faces & High density & 0.89 & 0.92 & 0.90 \\ 
MTCNN-Faces & Random & 0.94 & 0.62 & 0.75 \\ 
OpenPose-Faces & High density & 0.78 & 0.93 & 0.84 \\ 
OpenPose-Faces & Random & 0.72 & 0.80 & 0.76 \\ 
ViolaJones-Faces & High density & 0.96 & 0.44 & 0.60 \\ 
ViolaJones-Faces & Random & 0.44 & 0.38 & 0.41 \\ 
OpenPose-Wrists & High density & 0.66 & 1.00 & 0.79 \\ 
OpenPose-Wrists & Random & 0.88 & 0.29 & 0.44 \\ 
\hline
\end{tabular}
\caption{Detector performance on both high density samples (where proportion of targets detected was high) and random samples (where frames were randomly selected). P, R, and F denote precision, recall, and F-score, respectively.} 
\vspace{-1em}
\end{table}


# Results
First, we report developmental shifts in infants' posture and their orientation relative to their caregiver, consistent with previous literature [@adolph2006motor; @franchak2017see]. Then, we examine how these changes influence children's visual access to faces and wrists/hands across this developmental time range, examining the relative contributions of age vs. postural developments. We also explore how these changes impact the accessibility of faces and wrists/hands during labeling events, both when parents were labeling familiar objects (e.g., 'cat') as well as novel objects (e.g., 'zem'). Finally, we apply the same automated detection method to a egocentric video dataset collected by a different lab [@franchak2017see] during which children's in-the-moment posture was annotated.

```{r calcPostAndOrient}
## Calculate distributions of infants spent in each category separately for posture / orientation
ages_inc_subs <- inc_subs %>%
  select(age.at.test, age.grp, subid)

orientation <- d %>%
  filter(!is.na(orientation)) %>%
  group_by(age.grp, subid, orientation) %>%
  summarise(time = sum(dt, na.rm=TRUE)) %>%
  mutate(prop.time = time/sum(time)) %>%
  group_by(subid, age.grp, orientation) %>%
  left_join(ages_inc_subs)

posture <- d %>%
  filter(!is.na(posture)) %>%
  group_by(age.grp, subid, posture) %>%
  summarise(time = sum(dt, na.rm=TRUE)) %>%
  mutate(prop.time = time/sum(time)) %>%
  group_by(subid, age.grp, posture) %>%
  left_join(ages_inc_subs)
```

```{r setupPosOrientPlot}
## Make plots of time spent in each orientation / posture
orientation_labels <- c(
  behind ='Caregiver behind',
  close = 'Caregiver close',
  far = 'Caregiver far')

posture_labels = c(
  carry = 'Infant carried',
  prone = 'Infant prone',
  sit = 'Infant sitting',
  stand ='Infant standing'
)

pos_plot <- ggplot(posture, aes(x = age.at.test, y = prop.time, col = posture, size=time)) + 
  geom_point(alpha=.6) +
  ylab("Proportion Time") + 
  xlab("Age (months)") + 
  theme_few(base_size=plot_size) +
  ylim(0,1) +
  facet_grid(~posture, labeller = labeller(posture = posture_labels)) +
  geom_smooth(span=5, alpha=.2) +
  scale_x_continuous(breaks=c(8,12,16)) +
  scale_color_manual(name = "Posture", values=c("#8878cc","#0099cc","#339966","#eb7928")) +
  theme(legend.position = 'none')

orient_plot <- ggplot(orientation, aes(x = age.at.test, y = prop.time, col = orientation, size=time)) + 
  geom_point(alpha=.8) +
  ylab("Proportion Time") + 
  xlab("Age (months)") + 
  theme_few(base_size=plot_size) +
  ylim(0,1) +
  facet_grid(~orientation,  labeller = labeller(orientation= orientation_labels)) +
  geom_smooth(span=10, alpha=.2) +
  scale_x_continuous(breaks=c(8,12,16)) +
  scale_color_manual(name = "CG", values=c("#171615","#5c5753","#99928d")) +
  theme(legend.position = 'none')
```

```{r posture, fig.env = "figure", fig.pos = "H", echo=FALSE, out.width = "\\textwidth", fig.cap = "Proportion of time spent by each infant in different postures and orientations relative to their caregivers (CG); times where posture was not codable are ommitted for visualization purposes"}
cowplot::plot_grid(pos_plot, orient_plot, nrow=2)
```

```{r setupIndivPosOrientPlot}
### Calculate individual amounts of time spent in each orientation by subject
posture_by_orientation <- d %>%
  filter(!is.na(posture)) %>%
  # filter(!is.na(orientation)) %>%
  group_by(subid, posture, orientation) %>%
  summarise(time = sum(dt, na.rm=TRUE)) %>%
  group_by(subid) %>%
  mutate(prop.time = time/sum(time)) %>%
  mutate(pos_orient = paste0('Infant ',posture, ', CG ', orientation)) 
posture_by_orientation <- posture_by_orientation %>%
  left_join(ages_inc_subs) %>%
  ungroup() %>%
  mutate(subid = fct_reorder(subid, age.at.test))
```

```{r indivPosOrient,  fig.env = "figure", fig.pos = "H", echo=FALSE, out.width = "\\textwidth", fig.width=8, fig.height=4, fig.cap = "Proportion of time spent by each infant in different postures and orientations relative to their caregivers (CG); times where posture was not codable are ommitted for visualization purposes"}

## should be a two column figure
ggplot(posture_by_orientation, aes(x = subid, y = prop.time)) +
  geom_col(aes(fill = pos_orient), position=position_stack()) +
  ylab("Proportion Time") +
  xlab("Individual subjects (ordered by age)") +
  ylim(0,1) +
  theme_few(base_size=plot_size) +
  theme(axis.text.x=element_blank(),axis.ticks.x=element_blank()) +
  scale_fill_manual(name = "Posture and CG Orientation", values=c("#8878cc","#006080", "#0099cc", "#99e6ff","#87b5c4", "#339966","#66cc99", "#b3e6cc", "#98a69f", "#e67300","#ffa64d", "#ffe6cc","#d1b673"))
```

## Changes in Posture and Orientation
How do childen's in-the-moment posture and orientation change across age? In our dataset, the proportion of time infants spent sitting decreased with age, and the proportion of time infants spent standing increased with infants' age. As children got older, their locomotive abilities allowed them to become more independent. Both 8-month-olds and 12-month-olds spent relatively equivalent amounts of time lying/crawling (i.e., 'prone') which was markedly decreased in the 16-month-olds, who spent most of their time sitting or standing (see Figure \ref{fig:posture}). We also observed changes in children's orientation relative to their caregivers: the 8-month-olds spent more time with their caregiver behind them supporting their sitting positions than did children at other ages (see Figure \ref{fig:posture}). However, we also saw considerable variability across children: some infants spent almost their entire time sitting at a close distance from their caregiver, whereas others showed more considerable variability (see Figure \ref{fig:indivPosOrient}). 

## Changes in Access to Faces and Hands
We first examined the proportion of face and hand detections as a function of children's age without considering their posture (see Figure \ref{fig:detByAge}); here, we report face and wrist detections using OpenPose, although the same pattern of results for face detections was found using the outputs from MTCNN.
While faces tended to be in the field-of-view overall more often than hands, children's head-mounted cameras were angled slightly upward to capture the presence of faces, and hand detections suffered from somewhat lower recall than face detections. Going forward, we thus analyze differences in the relative proportion of faces or hands in view as a function of age, posture, and orientation, rather than comparing them directly. Overall, we observed that 12-month-olds appeared to have visual access to slightly fewer faces than 8 or 16-month-olds, creating a slight U-shaped function in face detections; conversely, hand detections were showed a slight increase across this age range, as reported in prior literature [@fausey2016].

However, these age-related trends were much smaller than the effect of infant's postural developments on children's visual access to faces and hands. Children's in-the-moment posture was a major factor both in how many faces and hands were in view during the play session, as was their orientation relative to their caregiver. Infants who were sitting or standing had more faces in view than infants who were lying down/crawling (i.e. prone), which was most frequent among 12-month-olds relative to the other age groups  (Figure \ref{fig:detByPosOrient}). When caregivers were behind their children, supporting their children's sitting or standing positions, children saw fewer faces and wrists. In particular, children who were sitting or in front of their caregiver had a high proportion of faces and hands in their field of view (Figure \ref{fig:detByPosOrient}). 



```{r runModels, eval=TRUE, include=TRUE}
library(MuMIn)

#wrangle data
## Using MTCNN detections for GLM since it was the best
dGLM_faces <- d %>%
  mutate(subid = as.factor(subid)) %>%
  group_by(subid, posture, orientation) %>%
  summarize(countFaces=sum(faceOP==TRUE), countNotFaces = sum(faceOP==FALSE), age.at.test=age.at.test[1]) %>%
  ungroup()

dGLM_wrists <- d %>%
  mutate(subid = as.factor(subid)) %>%
  group_by(subid, posture, orientation) %>%
  summarize(countWrists=sum(wristOP==TRUE), countNotWrists = sum(wristOP==FALSE), age.at.test=age.at.test[1]) %>%
  ungroup()

### FACES

# Age as fixed effect, full random effects
faceGLM_age_only <- glmer(cbind(countFaces,countNotFaces) ~  scale(age.at.test) +
(posture + orientation | subid), dGLM_faces, family = "binomial", control=glmerControl(optCtrl=list(maxfun=20000),optimizer=c("bobyqa")))

# Age and posture and orientation as fixed effects, full random effects
faceGLM_no_int <- glmer(cbind(countFaces,countNotFaces) ~ posture + orientation + scale(age.at.test) +  (posture + orientation | subid), dGLM_faces, family = "binomial", control=glmerControl(optCtrl=list(maxfun=20000),optimizer=c("bobyqa")))

# Age and posture * orientation as fixed effects, full random effects
faceGLM_full <- glmer(cbind(countFaces,countNotFaces) ~ posture * orientation + scale(age.at.test) +  (posture + orientation | subid), dGLM_faces, family = "binomial", control=glmerControl(optCtrl=list(maxfun=20000),optimizer=c("bobyqa")))

### WRISTS
# Age as fixed effect, full random effects
wristGLM_age_only <- glmer(cbind(countWrists,countNotWrists) ~ scale(age.at.test) +  (posture + orientation | subid), dGLM_wrists, family = "binomial", control=glmerControl(optCtrl=list(maxfun=20000),optimizer=c("bobyqa")))

wristGLM_no_int <- glmer(cbind(countWrists,countNotWrists) ~ orientation + posture + scale(age.at.test) +  (posture + orientation | subid), dGLM_wrists, family = "binomial", control=glmerControl(optCtrl=list(maxfun=20000),optimizer=c("bobyqa")))

# Age and posture and orientation as fixed effects, full random effects
wristGLM_full <- glmer(cbind(countWrists,countNotWrists) ~ orientation * posture + scale(age.at.test) +  (posture + orientation | subid), dGLM_wrists, family = "binomial", control=glmerControl(optCtrl=list(maxfun=20000),optimizer=c("bobyqa")))
```

```{r eval=FALSE}
## output
cleanedNames_Int = c("Intercept","Sit","Stand","Close","Far","Age (Scaled)", "Sit*Close", "Stand*Close","Sit*Far","Stand*Far")

#
faceGLMOut=(summary(faceGLM_full))
rownames(faceGLMOut$coefficients)<-cleanedNames_Int
#
wristGLMOut=(summary(wristGLM_full))
rownames(wristGLMOut$coefficients)<-cleanedNames_Int
#

###
xtable(wristGLMOut$coefficients, digits=c(2,2,2,2,3),"Model coefficients from a generalized linear mixed models predicting the proportion of wrists seen by infants.")

xtable(faceGLMOut$coefficients, digits=c(2,2,2,2,3),"Model coefficients from a generalized linear mixed models predicting the proportion of faces seen by infants.")
```

```{r}
wrists_age_only = r.squaredGLMM(wristGLM_age_only)
wrists_full= r.squaredGLMM(wristGLM_full)

faces_age_only =  r.squaredGLMM(faceGLM_age_only)
faces_full =r.squaredGLMM(faceGLM_full)

# wrists_no_int= r.squaredGLMM(wristGLM_no_int)
# faces_no_int =  r.squaredGLMM(faceGLM_no_int)
```

These trends were quantified using two generalized linear mixed-effect models estimating the proportion of faces and hands that were in view, with orientation, posture, their interaction, and scaled participant's age as fixed effects, and with random slopes for infants' orientation and posture. A summary of the coefficients of the models can be found in Tables 2 and 3, confirming that infants who were sitting/standing and in front of their caregivers saw the most faces and hands.  When modeling the proportion of faces seen, age was not a significant predictor; however, age remained a significant predictor when modeling the proportion of hands seen by infants. 

We directly examined the contributions of posture and orientation by fitting a reduced version of the full model [@nakagawa2013general] without their fixed effects (both models were run with the maximal random effects structure). The fixed effects in a model with only the age of the participants accounted for relatively little variance in the proportion of faces (marginal $R^2$ < `r round(faces_age_only[1,1],5)`) or hands in view  (marginal $R^2$ = `r round(wrists_age_only[1,1],3)`). However, when adding children's posture and orientation to their caregiver to the model (and their interaction) the marginal $R^2$ were higher for both faces (marginal $R^2$ = `r round(faces_full[1,1],3)`) and wrists (marginal $R^2$ = `r round(wrists_full[1,1],3)`).  

Overall, these results suggest that infants' visual access to social information is largely modulated by their posture and orientation to their caregiver, which is in turn a function of their general locomotor development. Nonetheless, these results also suggest that infants may still increase their attention to hands throughout this age range as they continue to learn about objects, their functions, and their names. 

## Access to Faces and Hands During Labeling Events
Our play session was designed to provide parents with opporunities to label objects--both familiar and novel--such that we could examine whether children sought out different kinds of social information around naming events. We thus explored how face and hand detections changed during object labeling events, analyzing a four-second window around each labeling event (e.g., "Look at the [zem]!"). Every utterance of one of the labelled objects (e.g., 'ball') was counted as a 'labeling event'; timestamps of the beginning of each word were hand-annotated from transcripts ad synchronized with the frame-by-frame detections. Overall, we did not find major differences in face/hand detections during naming events relative to baseline, either for novel objects or for familiar objects (see Figure \ref{fig:detByNaming}). This was true both when we analyzed naming rates with or without taking into account children's posture and orientation relative to their caregiver. These results suggest that either children did not actively seek out social information during this particular play session around naming events, perhaps because were a limited number of possible referents at any given time.


\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & z value & Pr($>$$|$z$|$) \\
  \hline
Intercept & -3.35 & 0.17 & -19.49 & 0.000 \\
  Sit & 0.31 & 0.20 & 1.58 & 0.114 \\
  Stand & -0.02 & 0.20 & -0.12 & 0.905 \\
  Close & 0.03 & 0.19 & 0.15 & 0.882 \\
  Far & 0.46 & 0.24 & 1.93 & 0.053 \\
  Age (Scaled) & 0.14 & 0.11 & 1.22 & 0.223 \\
  Sit*Close & 0.92 & 0.07 & 13.31 & 0.000 \\
  Stand*Close & 1.30 & 0.08 & 16.02 & 0.000 \\
  Sit*Far & 0.54 & 0.07 & 7.37 & 0.000 \\
  Stand*Far & 1.24 & 0.09 & 14.18 & 0.000 \\
   \hline
\end{tabular}
\caption{Model coefficients from a generalized linear mixed models predicting the proportion of faces seen by infants (as detected by OpenPose).}
\end{table}


\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & z value & Pr($>$$|$z$|$) \\
  \hline
Intercept & -4.28 & 0.23 & -18.67 & 0.000 \\
  Sit & 0.54 & 0.19 & 2.90 & 0.004 \\
  Stand & 0.61 & 0.23 & 2.63 & 0.009 \\
  Close & 0.67 & 0.22 & 3.05 & 0.002 \\
  Far & 0.33 & 0.24 & 1.35 & 0.176 \\
  Age (Scaled) & 0.45 & 0.13 & 3.35 & 0.001 \\
  Sit*Close & 0.34 & 0.09 & 3.93 & 0.000 \\
  Stand*Close & 0.73 & 0.09 & 7.92 & 0.000 \\
  Sit*Far & -0.15 & 0.11 & -1.39 & 0.165 \\
  Stand*Far & 1.32 & 0.11 & 11.70 & 0.000 \\
   \hline
\end{tabular}
\caption{Model coefficients from a generalized linear mixed models predicting the proportion of wrists seen by infants (as detected by OpenPose).}
\end{table}

```{r setupdetsByAgePlot}
detectionsByAge <- dlong %>%
  filter(detectorType!="MTCNN-Faces") %>% # for brevity
  filter(detectorType!="ViolaJones-Faces") %>% # low performance
  filter(!is.na(age.at.test)) %>% # filters NAs for combos of orientation/posture that were empty (see merge_data.Rmd)
  group_by(subid, age.at.test, detectorType) %>%
  summarise(prop_detected = mean(detection), num_detections = length(detection)) %>%
  left_join(inc_subs) %>%
  mutate(len = as.numeric(len.min)) ## allows us to scale dots by length of video
```

```{r detByAge, fig.env = "figure", fig.pos = "H", echo=FALSE, out.width = "\\textwidth", fig.height=3, fig.cap = "Proportion of faces (left) and wrists (right) detected by the OpenPose model as a function of child's age. Larger dots indicate children who had longer play sessions and thus for whom there was more data."}

ggplot(detectionsByAge, aes(x = age.at.test, y = prop_detected, color = detectorType, size=num_detections)) +
  theme_few(base_size=plot_size) + 
  geom_point(alpha=.5) +
  geom_smooth(size=1, alpha=.3, span=10) +
  scale_x_continuous(breaks=c(8,12,16)) +
  scale_y_continuous(breaks=seq(0,.60,.30), limits=c(0, .6)) +
  scale_size_continuous(name = "Frames") +
  labs(y = "Proportion detected", x = element_blank()) +
  # scale_colour_discrete(name = element_blank()) +
  theme(legend.position="none") +
  ggthemes::scale_color_solarized(name = "") + 
  facet_grid(~detectorType) 

```

```{r setupdetByPosOrientPlot}
### Make dataframes for main plot (detections by orientation & posture)
# filter out detectors we aren't plotting
# filter out detections w/o posture/orientation codings (wasn't always in view) -- this elimiantes "carry"
dlongPlot <-dlong %>%
  filter(detectorType!="MTCNN-Faces") %>%
  filter(detectorType!="ViolaJones-Faces") %>%
  filter(!is.na(posture)) %>%
  filter(!is.na(orientation)) 

# pop out of actual ages of included subs
ages_inc_subs <- inc_subs %>%
  select(subid, age.at.test, age.grp)

# raw data
detectionsByPostureAndOrient <- dlongPlot %>%
  group_by(age.grp, posture, orientation, subid, detectorType) %>%
  summarise(prop_detected = mean(detection, na.rm=TRUE), num_detections = length(detection)) %>%
  left_join(ages_inc_subs) %>%
  mutate(age.at.test = as.numeric(age.at.test))

# with CIs
detectionsByPostureAndOrientByGroup <- dlongPlot %>%
  group_by(age.grp, orientation, posture, subid, detectorType) %>%
  summarise(prop_detected = mean(detection), num_detections = length(detection)) %>%
  group_by(age.grp, orientation, posture, detectorType) %>%
  multi_boot_standard(col = 'prop_detected')

```

```{r detByPosOrient, fig.env="figure", out.width = "\\textwidth", fig.pos = "H", fig.align = "center", fig.width=8, fig.height=6, fig.cap = "Proportion of face / wrist detections by children's age, their posture, and their caregivers orientation. Data points are scaled by the amount of time spent in each orientation/posture combination; times when posture/orientation annotatinos were unavaliable or the infant was carried are not plotted. Error bars represent 95\\% bootstrapped confidence intervals"}

# better labels 
orientation_labels <- c(
  behind ='Caregiver behind',
  close = 'Caregiver close',
  far = 'Caregiver far')

posture_labels = c(
  prone = 'Infant prone',
  sit = 'Infant sitting',
  stand ='Infant standing'
)

 ## plot data all together now, with CIs and individual points from each infant
ggplot(detectionsByPostureAndOrientByGroup, aes(x = age.grp, y = mean, col = detectorType)) + 
  # plot raw data
  geom_point(data=detectionsByPostureAndOrient, alpha=.2, aes(x = age.at.test, y=prop_detected, size=num_detections)) +
  # plot trend lines & SEs of trends based on raw data
  geom_smooth(data=detectionsByPostureAndOrient, aes(x = age.at.test, y=prop_detected, weight = num_detections), alpha=0.1, size=0, span=5) +
  stat_smooth(data=detectionsByPostureAndOrient, aes(x = age.at.test, y=prop_detected, weight = num_detections), geom="line", alpha=0.5, size=1, span=5) +
  # plot means and CIs based on age groups
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width=1), alpha=.9) +
  # fix up the plot to look nice
  labs(y = "Proportion detections", x = "Age (months)") +
  theme_few(base_size=plot_size) +
  theme(legend.position="bottom", legend.text=element_text(size = 12), legend.margin=margin(t=0, r=0, b=0, l=0, unit="cm"), legend.title=element_text(size = 12)) +
  ggthemes::scale_color_solarized(name = "") + 
  scale_size_continuous(name = 'Frames') +
  scale_y_continuous(breaks=seq(0,.60,.30), limits=c(0, .6)) +
  scale_x_continuous(breaks=seq(8, 16, 4), limits=c(7, 18)) +
  facet_grid(orientation ~ posture, labeller = labeller(posture = posture_labels, orientation= orientation_labels)) 
  
```

```{r summarizeNamings}
source("helper.R")
## summarize detections around naming events
aroundNamings_Wrists <- d %>%
  group_by(subid) %>%
  mutate(detections = wristOP) %>%
  do(summarize.naming(.))

aroundNamings_Faces <- d %>%
  group_by(subid) %>%
  mutate(detections = faceOP) %>%
  do(summarize.naming(.))
```

```{r afterNamingsExplore, eval=FALSE, include=FALSE, echo=FALSE}
afterNamings_Faces <- d %>%
  group_by(subid) %>%
  mutate(detections = faceOP) %>%
  do(summarize.naming((.),window = c(0,4)))

detectionsByGroup <- dlongPlot %>%
  group_by(age.grp, subid, detectorType) %>%
  summarise(prop_detected = mean(detection), num_detections = length(detection)) %>%
  group_by(age.grp, detectorType) %>%
  multi_boot_standard(col = 'prop_detected')

afterNamings_Detections_BySub_Faces <- afterNamings_Faces %>%
  group_by(subid, familiarity, age.grp) %>%
  summarize(prop_detected = mean(detections, na.rm=TRUE))

afterNamings_Detections_Grouped_Faces <- afterNamings_Detections_BySub_Faces %>%
  group_by(familiarity, age.grp) %>%
  multi_boot_standard(col = 'prop_detected')

faces_afternamings_plot <- ggplot(afterNamings_Detections_Grouped_Faces, aes(x=age.grp, y=mean, color=familiarity)) +
  # plot baseline
  geom_pointrange(data = (detectionsByGroup %>% filter(detectorType == 'OpenPose-Faces')), aes(ymin = ci_lower, ymax = ci_upper, color='baseline'), position = position_dodge(width=3), alpha=.9) +
  # plot averages by familiarity 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width=3), alpha=.9) +
  # plot familairity detections for each sub
  geom_point(data=afterNamings_Detections_BySub_Faces, aes(x=age.grp, y=prop_detected), position = position_dodge(width=3), alpha=.2) +
  theme_few() +
  labs(y = "Prop. face detections", x = "Age (months)") +
  theme_few(base_size=plot_size) +
  ylim(0,.6) +
  theme(legend.position="bottom", legend.text=element_text(size = 12), legend.margin=margin(t=0, r=0, b=0, l=0, unit="cm"), legend.title=element_text(size = 14)) +
  scale_colour_viridis(name = 'Object familiarity', discrete=TRUE, option='A', begin=.2, end=.8) +
  scale_x_continuous(breaks=seq(8, 16, 4), limits=c(7, 18)) +
  scale_size_continuous(name="# of naming events")

```

```{r setUpNamingsPlot}
### Analyze detections around naming events (without posture/orientation) 

### first by faces
aroundNamings_Detections_BySub_Faces <- aroundNamings_Faces %>%
  group_by(subid, familiarity, age.grp) %>%
  summarize(prop_detected = mean(detections, na.rm=TRUE))

aroundNamings_Detections_Grouped_Faces <- aroundNamings_Detections_BySub_Faces %>%
  group_by(familiarity, age.grp) %>%
  multi_boot_standard(col = 'prop_detected')

### now by wrists 
aroundNamings_Detections_BySub_Wrists <- aroundNamings_Wrists %>%
  group_by(subid, familiarity, age.grp) %>%
  summarize(prop_detected = mean(detections, na.rm=TRUE))

aroundNamings_Detections_Grouped_Wrists <- aroundNamings_Detections_BySub_Wrists %>%
  group_by(familiarity, age.grp) %>%
  multi_boot_standard(col = 'prop_detected')

### get individual points
detectionsByGroup <- dlongPlot %>%
  group_by(age.grp, subid, detectorType) %>%
  summarise(prop_detected = mean(detection), num_detections = length(detection)) %>%
  group_by(age.grp, detectorType) %>%
  multi_boot_standard(col = 'prop_detected')
```

```{r detByNamingPlotPanels}
## Plot faces/wrists around namings by age

faces_aroundnamings_plot <- ggplot(aroundNamings_Detections_Grouped_Faces, aes(x=age.grp, y=mean, color=familiarity)) +
  # plot baseline
  geom_pointrange(data = (detectionsByGroup %>% filter(detectorType == 'OpenPose-Faces')), aes(ymin = ci_lower, ymax = ci_upper, color='baseline'), position = position_dodge(width=3), alpha=.9) +
  # plot averages by familiarity 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width=3), alpha=.9) +
  # plot familairity detections for each sub
  geom_point(data=aroundNamings_Detections_BySub_Faces, aes(x=age.grp, y=prop_detected), position = position_dodge(width=3), alpha=.2) +
  theme_few() +
  labs(y = "Prop. face detections", x = "Age (months)") +
  theme_few(base_size=plot_size) +
  ylim(0,.6) +
  theme(legend.position="bottom", legend.text=element_text(size = 12), legend.margin=margin(t=0, r=0, b=0, l=0, unit="cm"), legend.title=element_text(size = 14)) +
  scale_colour_viridis(name = 'Object familiarity', discrete=TRUE, option='A', begin=.2, end=.8) +
  scale_x_continuous(breaks=seq(8, 16, 4), limits=c(7, 18)) +
  scale_size_continuous(name="# of naming events")

## Faces
wrists_aroundnamings_plot <- ggplot(aroundNamings_Detections_Grouped_Wrists, aes(x=age.grp, y=mean, color=familiarity)) +
  # plot baseline
  geom_pointrange(data = (detectionsByGroup %>% filter(detectorType == 'OpenPose-Wrists')), aes(ymin = ci_lower, ymax = ci_upper, color='baseline'), position = position_dodge(width=3), alpha=.9) +
  # plot averages by familiarity 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width=3), alpha=.9) +
  # plot familairity detections for each sub
  geom_point(data=aroundNamings_Detections_BySub_Wrists, aes(x=age.grp, y=prop_detected), position = position_dodge(width=3), alpha=.2) +
  theme_few() +
  labs(y = "Prop. wrist detections", x = "Age (months)") +
  theme_few(base_size=plot_size) +
  ylim(0,.6) +
  theme(legend.position="bottom", legend.text=element_text(size = 12), legend.margin=margin(t=0, r=0, b=0, l=0, unit="cm"), legend.title=element_text(size = 14)) +
  scale_colour_viridis(name = 'Object familiarity', discrete=TRUE, option='A', begin=.2, end=.8) +
  scale_x_continuous(breaks=seq(8, 16, 4), limits=c(7, 18)) +
  scale_size_continuous(name="# of naming events")

```

```{r detByNaming, fig.env="figure", out.width = "\\textwidth", fig.pos = "H", fig.align = "center", fig.width=8, fig.height=4, fig.cap = "Proportion of face / wrist detections during naming events (+/- 2 seconds around label)for familiar and novel objects; these rates are put into context relative to baseline Error bars represent 95\\% bootstrapped confidence intervals."}
cowplot::plot_grid(faces_aroundnamings_plot, wrists_aroundnamings_plot)
```





## Extension to Franchak et al., 2017
In the present work, we found that infant's in-the-moment posture changed with their age, as did infants' orientation relative their caregiver. In a related study with 12-month-olds, @franchak2017see also that children's in-the-moment posture changed the amount of time infants spent looking at faces. Here, we sought to replicate the findings from @franchak2017see using our automated methodology (OpenPose detections), using the footage from their head-mounted cameras [@simon2015databrary]. 

This dataset differed from the present in two key ways that could present a challenge for our automated methodology. First, the environment that infants were immersed in with their caregivers (and experimenters)  was much larger and more varied than the play room used in the present dataset, containing multiple structures and toys in different parts of the room for infants to explore, unlike the  room used for the present dataset which was relatively small (approximately 10' x 10'). Thus, our automated methodology could fail to generalize to scenes from these more complex environments, where detecting faces and hands could arguably be a much harder task. Second, @franchak2017see used a head-mounted eye-tracker, finding that children's posture affected where children were looking within their visual field. Nonetheless, if children are often orienting their heads towards where they are allocating their attention [@yoshida2008] then we should expect to find the same pattern of results only when analyzing the information in view.
```{r loadFranchakData}
# loads pos_detections used below for franchak dataset
load(file = '../data/franchak_dataset/preprocessed/franchak_dataset_detections.RData')
```

```{r processFranchak}
faces_by_pos  <- pos_detections %>%
  group_by(sub_id, infant_posture_label) %>%
  summarize(prop_faces = mean(face_openpose)) %>%
  group_by(infant_posture_label) %>%
  multi_boot_standard(col = 'prop_faces')

faces_by_pos_by_sub  <- pos_detections %>%
  group_by(sub_id, infant_posture_label) %>%
  summarize(prop_faces = mean(face_openpose), faces = sum(face_openpose), no_faces = sum(face_openpose==0))

hands_by_pos  <- pos_detections %>%
  group_by(sub_id, infant_posture_label) %>%
  summarize(prop_hands = mean(hand_openpose)) %>%
  group_by(infant_posture_label) %>%
  multi_boot_standard(col = 'prop_hands')

hands_by_pos_by_sub  <- pos_detections %>%
  group_by(sub_id, infant_posture_label) %>%
  summarize(prop_hands = mean(hand_openpose), hands = sum(hand_openpose), no_hands = sum(hand_openpose==0))
```

```{r statsFranchak}
# glmers for franchak dataset
posture_model_faces_franchak <- glmer(cbind(faces,no_faces) ~ infant_posture_label + (1|sub_id), data=faces_by_pos_by_sub, family="binomial")

posture_model_hands_franchak <- glmer(cbind(hands,no_hands) ~ infant_posture_label + (1|sub_id), data=hands_by_pos_by_sub, family="binomial")
```

```{r plotPanelsFranchak}
faces_by_pos_plot <- ggplot(faces_by_pos, aes(x=infant_posture_label, y=mean, col=infant_posture_label)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
   geom_point(data=faces_by_pos_by_sub, aes(x=infant_posture_label, y=prop_faces, col=infant_posture_label), alpha=.1) +
  geom_line(data=faces_by_pos_by_sub, aes(x=infant_posture_label, y=prop_faces, col=infant_posture_label, group=sub_id), alpha=.1) +
  theme_few(base_size=plot_size) +
  ylab('Prop faces detected') +
  xlab('Infant posture') + 
  ylim(c(0,.3)) +
  theme(legend.position = 'none') +
  scale_color_manual(name = "Posture", values=c("#0099cc","#339966","#eb7928")) 

hands_by_pos_plot <- ggplot(hands_by_pos, aes(x=infant_posture_label, y=mean, col=infant_posture_label)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  geom_point(data=hands_by_pos_by_sub, aes(x=infant_posture_label, y=prop_hands, col=infant_posture_label), alpha=.1) +
  geom_line(data=hands_by_pos_by_sub, aes(x=infant_posture_label, y=prop_hands, col=infant_posture_label, group=sub_id), alpha=.1) +
  theme_few(base_size=plot_size) +
  ylab('Prop hands detected') +
   xlab('Infant posture') + 
  ylim(c(0,.4)) +
  theme(legend.position = 'none') +
  scale_color_manual(name = "Posture", values=c("#0099cc","#339966","#eb7928")) 
```

```{r franchak, fig.env="figure", out.width = "\\textwidth", fig.pos = "H", fig.align = "center", fig.width=8, fig.height=4, fig.cap = "Proportion of face / wrist detections for 12-month-olds in Franchak et al., 2017 as a function of children's in-the-moment posture. Error bars represent 95 percent bootstrapped confidence intervals."}
cowplot::plot_grid(faces_by_pos_plot,hands_by_pos_plot, nrow=1)
```

Overall, we found convergence between our two methodologies: we replicated the main results from @franchak2017see, finding that the proportion of faces in view was greater when infants were sitting or standing vs. prone. We also found the same patterns of results with hands (i.e., wrist detections), though these were not originally annotated in @franchak2017see (see Figure \ref{fig:franchak}); results were validated with generalized mixed effect models as in our previous analysis. Interestingly, there was a relatively higher proportion of hands in view in @franchak2017see relative to the present dataset; this is likely mostly due to the fact that there were often multiple people in view: experimenters stayed in the room to film children and assist the caregiver, while in our present dataset the experimenters left the room for the majority of the play session. Nonetheless, we still found that posture modulated the proportion of hands in view, suggesting that this is a major factor that structures infants' access to visual information.

# General Discussion
How do postural and locomotive developments influence the social information that infants see? We used a head-mounted camera to explore how these emerging motoric abilities affect children's visual access to social information, here operationalized as the presence of the faces and hands of their caregiver. First, we found systematic changes around the first birthday in children's in-the-moment posture and their orientation relative to the caregivers; older children spent more time standing and less time sitting; older children's caregivers spent less time supporting their standing or sitting postures. Children's changing posture and orientation to their caregiver jointly shaped the amount of social information that was in their view during one-on-one play sessions with their caregivers: Children saw the most faces/hands when they were sitting or standing and close to their caregiver. Motor development appears to modulate how infants experience their visual world and the social information in it. 

However, we did not see differences in visual access to social information during naming events, suggesting that children did not change where they were looking when hearing a label for either a novel or familiar object. We see several potential explanations for this: first, other work [@yoshida2008; @yu2013joint; @yu2017hand], including @franchak2017see, has found that infants spend much more time looking at the toys vs. their caregiver's faces during these play sessions; infants may have thus been primarily interested in exploring these new toys rather than learning their names. Furthermore, moving their own neck upwards towards their caregiver still requires quite a physical effort at this age (especially when prone or sitting) and simply may not have been a priority for children in this context. A second factor is that this particular play session did not present many opportunities where children would need to use social cues to disambiguate referents. Indeed, there were only two possible referents in the room at a time—-and one of them was always a familiar category (i.e., car, kitty).

Broadly,these results show promise for the use of automated methods to detect the social information in the egocentric infant viewpoint during more naturalistic parent-child interactons. The use of this automated methodology allowed us to easily annotate the entirety of the dataset and analyze all of the frames without the need for hundreds of hours of human coding. Importantly, we also applied these same methods to an addiontal dataset that contained more variation in the type and structure of the play session, replicating and extending the effects of children's in-the-moment posture on visual access to social information [@franchak2017see]. 

Yet more work is needed to understand how these results relate to children's home experiences [@fausey2016]. Both this play session and that of @franchak2017see were relatively controlled interactions where caregivers are highly aware that they are being montiored by experimenters. Furthermore, as children grow and change, the activities in which they engage with their caregivers are likely to also vary, leading to differences in the distribution of contexts they experience that may not be captured in these one-on-one play sessions. Finally, the ability to walk is of course only of a cascade of changes in children's abilities and experiences, and this study captures only a cross-sectional slice of this broader, multifaceted trajectory. 

Understanding these changes and how they relate to one another has been a persistent challenge for developmental psychology, but the field of computer vision has advanced dramatically in recent years, creating a new generation of algorithmic tools. These tools deal better with noisier, more complicated datasets and extract richer information than previous system. We hope that these new tools can be leveraged to understand the changing infant perspective on the visual world, both in controlled experimental contexts and in children's home environments. By doing so, we believe that we can understand the implications of the changing infant perspective consequences for linguistic, cognitive, and social development.

# Acknowledgements

Thanks to Kaia Simmons, Kathy Woo, and Aditi Maliwal for help in recruitment, data collection, and annotation. This work was funded by a Jacobs Foundation Fellowship to MCF, a John Merck Scholars award to MCF, and NSF #1714726 to BLL. An earlier version of this work was presented to the Cognitive Science Society in @frank2013 and @sanchez2018postural.



\newpage

# References
```{r create_r-references}
r_refs(file = "xsface.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
