---
title: "Developmental and postural changes in children's visual access to faces"
bibliography: xsface.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Alessandro Sanchez} \\ \texttt{author1@university.edu} \\ Department of Psychology \\ Stanford University
    \And {\large \bf Bria Long} \\ \texttt{bria@stanford.edu} \\ Department of Psychology \\ Stanford University
        \And {\large \bf Ally Kraus} \\ \texttt{bria@stanford.edu} \\ Department of Psychology \\ Stanford University
        \And {\large \bf Michael C. Frank} \\ \texttt{mcfrank@stanford.edu} \\ Department of Psychology \\ Stanford University}

abstract: 

    
keywords:
    "social cognition; face-perception; infancy; locomotion; head-cameras"
    
output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(knitr)
knitr::opts_chunk$set(fig.width=6, fig.height=5, fig.crop = FALSE, 
                      fig.path='figs/', echo=FALSE, warning = FALSE, 
                      cache=TRUE, message = FALSE, sanitize = TRUE)
library(readr)
library(dplyr)
library(tidyr)
library(xtable)
library(lubridate)
```

# Methods
## Participants
```{r echo=FALSE, include=FALSE}
all_subs <- read.csv("../data/demographics/all_participants.csv") %>%
  rename(subid = subID)
inc_subs <- read.csv("../data/demographics/demographics.csv")
all_subs$inc <- all_subs$reason=="included"

inc_subs$len.min <- sapply(strsplit(as.character(inc_subs$len),":"),
       function(x) {
         x <- as.numeric(x)
         x[1]+x[2]/60
       }
)

all_subs %>% 
  group_by(reason) %>%
  summarise(n = n()) %>% 
  kable

all_subs %>%
  left_join(inc_subs) %>%
  group_by(age_group) %>%
  summarise(n = sum(inc), 
            percent_included = mean(inc),
            age = mean(age.at.test, na.rm=TRUE), 
            video_length = mean(len.min, na.rm=TRUE), 
            female = sum(gender=="female", na.rm=TRUE)) %>%
  xtable %>%
  print(digits = 2, include.rownames=FALSE,table.placement = "H", type="latex", comment = F)
```
Our final sample consisted of 36 infants and children, with 8 participants in three age groups: 8 months (6 females), 12 months (7 females), and 16 months (6 females). Participants were recruited from the surrounding community via state birth records, had no documented disabilities, and were reported to hear at least 80\% English at home. Demographics and exclusion rates are given in Table \ref{tab:pop}. 
\begin{table}[H]
\centering
\begin{tabular}{rrrrrr}
  \hline
 Group & N & \% incl. & Mean age & Videos length (min) \\ 
  \hline
   8 mo. &   12 & 0.46 & 8.71 & 14.41 \\ 
   12 mo. &  12 & 0.40 & 12.62 & 13.48 \\ 
   16 mo. &  12 & 0.31 & 16.29 & 15.00\\ 
   \hline
\end{tabular}
\caption{\label{tab:pop} Demographics by age group.}
\end{table}
To obtain this final sample, we tested `r length(all_subs$reason)`, excluding  `r sum(all_subs$reason!="included")` children for the following reasons:
`r sum(all_subs$reason=="HC technical error")` for technical issues related to the headcam,
`r sum(all_subs$reason=="would not wear camera")` for failing to wear the headcam,
`r sum(all_subs$reason=="less than 4 min HC footage")` for fewer than 4 minutes of headcam footage,
`r sum(all_subs$reason=="multiple people")` for having multiple adults present,
`r sum(all_subs$reason=="no CDI")` for missing CDI data,
`r sum(all_subs$reason=="no DV cam footage")` for missing scene camera footage,
`r sum(all_subs$reason=="infant crying")` for fussiness, and one excluded for sample symmetry. All inclusion decisions were made independent of the results of subsequent analyses.

## Head-mounted camera
We used a small, head-mounted camera ("headcam") that was constructed from a MD80 model camera attached to a soft elastic headband.Videos captured by the headcam were 720x480 pixels with 25 frames per second.Detailed instructions for creating this headcam can be found  at \url{http://babieslearninglanguage.blogspot.com/2013/10/how-to-make-babycam.html}. A fisheye lens was attached to the camera to cincrease the view angle from $32^{\circ}$ horizontal by $24^{\circ}$ vertical to $64^{\circ}$ horizontal by $46^{\circ}$ vertical (see Figure \ref{fig:headcam}, left). 

Even with the fish-eye lens, the vertical field of view of the camera is still considerably reduced compared to the child's approximate vertical field of view, which spans around 100--120$^{\circ}$ in the vertical dimension by 6-7 months of age [@mayer1988;@cummings1988]. As we were primarily interested in the presence of faces in the child's field of view, we chose to orient the camera upwards to capture the entirely of the child's upper visual field where the child is likely to see the faces of adults around them. This allowed us to maximize our chances of capturing faces that the child would have seen during the play session.

```{r image, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=2, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.label="headcam", fig.cap = "Field of view for three different headcam configurations, with the device we used in the middle. The lowest camera is pictured for comparison, but was not available until after our study was already in progress."}
img <- png::readPNG("images/viewangle.png")
grid::grid.raster(img)
```

## Procedure

First, all parents signed consent documentsin a waiting room  where children were fitted with the headcam. After the child was comfortable in the waiting room and with the experimenter, the experimenter placed the headcam on the child's head. If the child was uninterested in wearing the headcam or tried to take it off, the experimenter presented engaging toys to try to draw the child's focus away from the headcam [@yoshida2008].

After the child was comfortable with wearing the headcam, the child and caregiver were shown to a playroom for the free-play session--the focus of the current study. Parents were shown a box containing three pairs of novel and familiar objects (e.g., a ball and a feather duster, named a "zem"), and were instructed to play with the object pairs with their child one at a time, "as they typically would." All parents confirmed that their child had not previosuly seen the novel toys and were instructed to use the novel labels to refer to the novel toys.  

The experimenter then left the playroom for approximately 15 minutes, during which a tripod-mounted camera in the corner of the room recorded the session and the headcam captured video from the child's perspective.

## Data Processing and Annotation

\begin{figure*}
\includegraphics[width=6in]{images/framesample.pdf}
\caption{\label{fig:frames} Sample frames from the headcam videos for a child from each age group, selected because they featured successful face detections (green squares).}
\end{figure*}

All headcam videos were cropped to exclude the period of entry to the playroom and were automatically synchronized with the tripod-mounted videos using FinalCut Pro Software. These sessions yielded a substantial amount of video: a total of
`r round(sum(inc_subs$len.min), digits = 0)` minutes (almost a milion frames), with an average video length of
`r round(sum(inc_subs$len.min)/60, digits = 2)` minutes
(min = `r round(min(inc_subs$len.min), digits = 2)`, max = `r round(max(inc_subs$len.min), digits = 2)`).

### Posture and Orientation Annotation

We created a set of custom annotations that described the child's physical posture (e.g. standing) and the orientation of the caregiver relative to the child (e.g. far away). The child's posture was categorized as being held/carried, prone (crawling or lying), sitting, or standing. The caregiver's orientation was characterized as being close to the child, farther from the child, and a global category of caregiver behind the child. For the first two annotationes (close/far from the child), the caregiver could either be to the the front or to the side of the child. All annontations were made using OpenSHAPA/Datavyu software  [@adolph2012], and times when the child was out of view of the tripod camera was marked as uncodable and was excluded from these annotations. 

## Face Detection

An additional goal of the study was to measure the presence of caregivers' faces in the child's field of view (as approximated by the headcam). To avoid hand-annotating the size and position of faces in every frame of video, we tested two face detection systems. Sample frames from the video with successful detections are given in Figure \ref{fig:frames}.

### Face detection algorithms

### Detector evaluation

\begin{table}[t]
  \caption{Model performance on gold standard generalization training set dataset. P, R, and F denote precision, recall, and F-score for each of the two samples. \label{tab:model_eval}}
  \begin{center}
    \begin{tabular}{l|ccc|ccc}
      \hline
       &  \multicolumn{3}{c|}{High-density} &  \multicolumn{3}{c}{Random} \\
      % \null Model & P & R & F & P & R & F  \\
      \hline
      % Viola-Jones & .55  & .38 &  .45 & .89 & .74 & .81   \\
      % MTCNN & .86 & .78 & .81 & .93 & .76 & .83 \\
      % OpenPose & .86 & .78 & .81 & .93 & .76 & .83 \\
    \hline
    \end{tabular}
  \end{center}
\end{table}

# Results
We report results from three different sets of analyses. First, we explore developmental changes in posture and orientation in our dataset. Next, we explore how these changes affect access to faces and to hands, as measured using computater vision algorithms. Finally, we explore how these changes impact the accessibility of faces and hands during labeling events.


## Changes in Posture and Orientation

## Changes in Access to Faces

## Changes in Access to Hands


# Acknowledgements

Thanks to Kaia Simmons, Kathy Woo, Aditi Maliwal, and other members of the Language and Cognition Lab for help in recruitment, data collection, and annotation. This research was supported by a John Merck Scholars grant to MCF. An earlier version of this work was presented to the Cognitive Science Society in @frank2013. Please address correspondence to Michael C. Frank, Department of Psychology, Stanford University, 450 Serra Mall (Jordan Hall), Stanford, CA, 94305, tel: (650) 724-4003, email: \texttt{mcfrank@stanford.edu}.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
