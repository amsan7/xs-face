# install 
sdev
module load system && module load singularity
mkdir $SCRATCH/.singularity4
export SINGULARITY_CACHEDIR=$PI_SCRATCH /.singularity4
singularity pull docker://cmpe679/cmpe679:detectron # this takes a while!

# run
exit # get out of sdev shell
gpu_shell # hopefully you still have this command aliased
singularity exec --nv $SINGULARITY_CACHEDIR/cmpe679-detectron.simg bash -c 'cd /detectron/lib && python3 ../tests/test_spatial_narrow_as_op.py'

# you can also use `singularity shell —nv …` to easily explore the contents of container

export SINGULARITY_CACHEDIR=$PI_SCRATCH /.singularity
sbatch -p gpu --gres gpu:1 -t 20:00 --mail-type=FAIL --mail-user=bria@stanford.edu --wrap="singularity exec --nv $SINGULARITY_CACHEDIR/cmpe679-detectron.simg bash -c 'cd /detectron/lib && python3 ../tests/test_spatial_narrow_as_op.py'"

######NOTES

## Issues with protobuf compilers on sherlock
Tests failed when trying to run either the contained in PI_SCRATCH or the one I had built
Without gpu shells run into massive errors
With gpu shell get tests to work but run into protobuf compiler issues?
Errors out on "from google import .." -- no module named google
Permissions issues when trying to install this library and don't understand why

## Issues with cuda libraries on neuroai cluster -- need to install cudnn?
Installed conda, created a testing environment, all dependnecies...couldn't get caffe2 installation running

Solution - maybe build our own docker image and then port to sherlock? this seeems to be a huge messs.
Need shareable coe base and having detctron on the neuroai cluster doesn't seem to be a productive use of time in any way shape or form
